+ [Метрические Алгоритмы классификации](https://github.com/vinovatvolondemord/ML0/blob/master/README.md#%D0%BC%D0%B5%D1%82%D1%80%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B5-%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC%D1%8B-%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%86%D0%B8%D0%B8)
    + [Алгоритм	ближайших	соседей](https://github.com/vinovatvolondemord/ML0/blob/master/README.md#%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC%D0%B1%D0%BB%D0%B8%D0%B6%D0%B0%D0%B9%D1%88%D0%B8%D1%85%D1%81%D0%BE%D1%81%D0%B5%D0%B4%D0%B5%D0%B9)
+ [Байесовский классификатор](https://github.com/vinovatvolondemord/ML0/blob/master/README.md#%D0%B1%D0%B0%D0%B9%D0%B5%D1%81%D0%BE%D0%B2%D1%81%D0%BA%D0%B8%D0%B9-%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%82%D0%BE%D1%80) 
    - [Наивный Нормальный Байесовский Классификатор](https://github.com/vinovatvolondemord/ML0/blob/master/README.md#%D0%BD%D0%B0%D0%B8%D0%B2%D0%BD%D1%8B%D0%B9-%D0%BD%D0%BE%D1%80%D0%BC%D0%B0%D0%BB%D1%8C%D0%BD%D1%8B%D0%B9-%D0%B1%D0%B0%D0%B9%D0%B5%D1%81%D0%BE%D0%B2%D1%81%D0%BA%D0%B8%D0%B9-%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%82%D0%BE%D1%80)
## Метрические алгоритмы классификации
### Алгоритм	ближайших	соседей
Необходимо было реализовать	алгоритм __k__ ближайших соседей – __kNN__.	

Суть метода __kNN__ в том что для нашего обьекта __u__ мы рассматриваем его __k__ ближайших соседей и приписываем u к тому классу в котором больше соседей данного класса. 

Если мы возьмеи маленькое __k__ то алгоритм будет неустойчив к шуму, т.е. неустойчив к погрешностям (выбросам -- объектам, которые окружены объектами чужого класса), если же возьмем k большим то алгоритм будет слишком устойчивым и к примеру если мы возьмем всю выборку как __k__ ,то любую точку алгоритм будет всегда приписывать к тому классу в котором больше всего элементов.

Нужно найти оптимальное __k__  и для этого воспользуемся критерием скользящего контроля __LOO__.
Суть критеия в том что мы берем нашу выборку и рассматриваем по одному её обьекту. Каждый обьект мы классифицируем пор методу __kNN (kwNN)__ и сравниваем совпал ли класс нашего обьекта с тем который у нас был в выборке. Если нет, то мы прибавляем к счетчику 1. После того как мы классифицировали так всю выборку поделим наш счетчик на длину ввыборки. Получим вероятность возникновения ошибки при __k__. Остаеться лишь найти такое __k__ при котором значение этой вероятности минимально.

![](https://raw.githubusercontent.com/vinovatvolondemord/ML0/master/img/img1.PNG)

Ну и остается лишь классифицировать все точки на разные классы

![](https://raw.githubusercontent.com/vinovatvolondemord/ML0/master/img/img2.PNG)

Далее необходимо было реализовать алгоритм __k__ взвешенных	ближайших соседей – __kwNN__.	
Он отличается от __kNN__ тем что учитывает порядок соседей классифицируемого объекта, т.е. будет учитываться последовательность  весов, задающая вклад соседа при классификации объекта.
 ![](https://raw.githubusercontent.com/vinovatvolondemord/ML0/master/img/img3.PNG) ![](https://raw.githubusercontent.com/vinovatvolondemord/ML0/master/img/img4.PNG)

Функцию весов я взял у других
```
weightsKWNN = function(i, k)  (k + 1 - i) / k
```
## Байесовский классификатор
###  Наивный Нормальный Байесовский Классификатор
"Наивный" классификатор будет думать что все X имеют n признаков: x={eps1,eps2,...,epsn}

